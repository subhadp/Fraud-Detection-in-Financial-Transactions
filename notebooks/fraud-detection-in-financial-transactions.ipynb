{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9332838,"sourceType":"datasetVersion","datasetId":5655040}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Problem Statement**:  \nDevelop a machine learning model to detect fraudulent transactions in real-time, enhancing the security of financial systems.\n\n**Introduction**:  \nFraudulent financial transactions can lead to significant losses. Detecting fraud in real-time can enhance the security of financial systems.\n\n**Relevance**:  \nDetecting fraud in real-time is essential for financial institutions to protect their customers and assets.\n\n**Data Source**:  \nFinancial Transactions Dataset.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc  # Garbage collection\n\n# Deep learning libraries\nimport torch\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Machine learning libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, \n                             classification_report, roc_auc_score, \n                             roc_curve)\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans\n\n# Handling imbalanced datasets\nfrom imblearn.over_sampling import SMOTE\n\n# Hugging Face transformers for pre-trained models and tokenizers\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# XGBoost\nimport xgboost as xgb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-28T13:41:24.517648Z","iopub.execute_input":"2024-09-28T13:41:24.518311Z","iopub.status.idle":"2024-09-28T13:41:24.768218Z","shell.execute_reply.started":"2024-09-28T13:41:24.518253Z","shell.execute_reply":"2024-09-28T13:41:24.766192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Data load**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/financial-dataset/Synthetic_Financial_datasets_log.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:11.017633Z","iopub.execute_input":"2024-09-28T11:48:11.018220Z","iopub.status.idle":"2024-09-28T11:48:35.805089Z","shell.execute_reply.started":"2024-09-28T11:48:11.018163Z","shell.execute_reply":"2024-09-28T11:48:35.803437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1.1 Transaction Dataset Field Summary**","metadata":{}},{"cell_type":"code","source":"fields = {\n    \"Step\": \"Represents a unit of time in hours. The simulation spans 744 steps (equivalent to 31 days).\",\n    \"type\": \"The type of transaction, which includes categories like CASH-IN, CASH-OUT, DEBIT, PAYMENT, and TRANSFER.\",\n    \"amount\": \"The transaction amount in the local currency.\",\n    \"nameOrig\": \"The ID of the customer initiating the transaction.\",\n    \"oldbalanceOrg\": \"The balance of the customer before the transaction.\",\n    \"newbalanceOrig\": \"The balance of the customer after the transaction.\",\n    \"nameDest\": \"The ID of the recipient of the transaction.\",\n    \"oldbalanceDest\": \"The balance of the recipient before the transaction.\",\n    \"newbalanceDest\": \"The balance of the recipient after the transaction.\",\n    \"isFraud\": \"A binary flag indicating whether the transaction is fraudulent.\",\n    \"isFlaggedFraud\": \"A binary flag indicating whether the transaction was flagged as potentially fraudulent.\"\n}\n\nfor i, (field, description) in enumerate(fields.items(), start=1):\n    print(f\"{i}. {field}: {description}\")\n\n\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:35.806712Z","iopub.execute_input":"2024-09-28T11:48:35.807168Z","iopub.status.idle":"2024-09-28T11:48:35.823643Z","shell.execute_reply.started":"2024-09-28T11:48:35.807120Z","shell.execute_reply":"2024-09-28T11:48:35.822197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Exploratory Data Analysis (EDA)**","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:35.827167Z","iopub.execute_input":"2024-09-28T11:48:35.827696Z","iopub.status.idle":"2024-09-28T11:48:37.856171Z","shell.execute_reply.started":"2024-09-28T11:48:35.827633Z","shell.execute_reply":"2024-09-28T11:48:37.854908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:37.857910Z","iopub.execute_input":"2024-09-28T11:48:37.858442Z","iopub.status.idle":"2024-09-28T11:48:39.968358Z","shell.execute_reply.started":"2024-09-28T11:48:37.858382Z","shell.execute_reply":"2024-09-28T11:48:39.967115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:39.969969Z","iopub.execute_input":"2024-09-28T11:48:39.970388Z","iopub.status.idle":"2024-09-28T11:48:39.991569Z","shell.execute_reply.started":"2024-09-28T11:48:39.970344Z","shell.execute_reply":"2024-09-28T11:48:39.990147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['type'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:39.993352Z","iopub.execute_input":"2024-09-28T11:48:39.993881Z","iopub.status.idle":"2024-09-28T11:48:41.175799Z","shell.execute_reply.started":"2024-09-28T11:48:39.993819Z","shell.execute_reply":"2024-09-28T11:48:41.174466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['isFraud'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:41.177537Z","iopub.execute_input":"2024-09-28T11:48:41.177971Z","iopub.status.idle":"2024-09-28T11:48:41.253378Z","shell.execute_reply.started":"2024-09-28T11:48:41.177920Z","shell.execute_reply":"2024-09-28T11:48:41.252103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.0 Distribution of Transaction Types**","metadata":{}},{"cell_type":"code","source":"transaction_examples = {\n    \"CASH-IN\": [\"ATM Deposit\",\"Cash Deposit at Bank\",\"Check Deposit\"],\n    \"CASH-OUT\": [\"ATM Withdrawal\",\"Credit Card Cash Advance\",\"Withdrawal from Investment Account\"],\n    \"DEBIT\": [\"Retail Purchase\",\"Online Shopping\",\"Restaurant Payment\"],\n    \"PAYMENT\": [\"Utility Bill Payment\",\"Credit Card Bill Payment\",\"Subscription Payment\"],\n    \"TRANSFER\": [\"Internal Transfer\",\"External Transfer\", \"Peer-to-Peer Transfer\"]\n}\n\nfor i, (transaction_type, examples) in enumerate(transaction_examples.items(), start=1):\n    print(f\"{i}. {transaction_type}:\")\n    for j, example in enumerate(examples, start=1):\n        print(f\"   {i}.{j} {example}\")\n\nsns.countplot(x='type', data=data)\nplt.title('Distribution of Transaction Types')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:41.255154Z","iopub.execute_input":"2024-09-28T11:48:41.255638Z","iopub.status.idle":"2024-09-28T11:48:48.638165Z","shell.execute_reply.started":"2024-09-28T11:48:41.255591Z","shell.execute_reply":"2024-09-28T11:48:48.636928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.1 Analysis of Transaction Distribution During Business and Non-Business Hours: Insights on Fraudulent vs Non-Fraudulent Transactions**","metadata":{}},{"cell_type":"code","source":"data['hour_of_day'] = data['step'] % 24\ndata['day'] = data['step'] // 24\n\n# Create a new column to differentiate business hours (10 AM to 6 PM)\ndef categorize_business_hour(hour):\n    if 10 <= hour < 18:\n        return 'Business Hour (10AM-6PM)'\n    else:\n        return 'Non-Business Hour'\n\ndata['time_category'] = data['hour_of_day'].apply(categorize_business_hour)\n\n#Analyze the number of transactions in business and non-business hours\ntransaction_counts = data['time_category'].value_counts()\n\n#Analyze the number of fraudulent transactions in business and non-business hours\nfraud_counts = data[data['isFraud'] == 1]['time_category'].value_counts()\n\n#Plot the distribution of transactions during business and non-business hours\nplt.figure(figsize=(10, 6))\nsns.countplot(data=data, x='time_category', hue='isFraud', palette='Set2')\nplt.title('Transaction Count: Business vs Non-Business Hours (Fraud vs Non-Fraud)')\nplt.xlabel('Time Category')\nplt.ylabel('Number of Transactions')\nplt.legend(title='Fraud', loc='upper right', labels=['Non-Fraudulent', 'Fraudulent'])\nplt.show()\n\n# Display the counts for both overall transactions and fraudulent transactions\nprint(\"Overall Transactions Count:\")\nprint(transaction_counts)\n\nprint(\"\\nFraudulent Transactions Count:\")\nprint(fraud_counts)\n\ndata.drop(columns=['hour_of_day', 'day', 'time_category'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:48:48.643788Z","iopub.execute_input":"2024-09-28T11:48:48.644224Z","iopub.status.idle":"2024-09-28T11:49:01.674080Z","shell.execute_reply.started":"2024-09-28T11:48:48.644179Z","shell.execute_reply":"2024-09-28T11:49:01.672472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.2 Histogram data presentation of Amount**","metadata":{}},{"cell_type":"code","source":"data[['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']].hist(bins=20, figsize=(10, 8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:49:04.153287Z","iopub.execute_input":"2024-09-28T11:49:04.153671Z","iopub.status.idle":"2024-09-28T11:49:06.472965Z","shell.execute_reply.started":"2024-09-28T11:49:04.153629Z","shell.execute_reply":"2024-09-28T11:49:06.471575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.3 Fraud and non-fraud transaction type**","metadata":{}},{"cell_type":"code","source":"fraud_transactions = data[data['isFraud'] == 1]\nnon_fraud_transactions = data[data['isFraud'] == 0]\n\nprint(\"fraud_transactions\",fraud_transactions['type'].value_counts())\nprint(\"non_fraud_transactions\",non_fraud_transactions['type'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:49:06.474510Z","iopub.execute_input":"2024-09-28T11:49:06.474970Z","iopub.status.idle":"2024-09-28T11:49:08.383320Z","shell.execute_reply.started":"2024-09-28T11:49:06.474918Z","shell.execute_reply":"2024-09-28T11:49:08.381941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.4 Count of fraudulent transactions based on Merchant and Customer**","metadata":{}},{"cell_type":"code","source":"fraud_data = data[data['isFraud'] == 1]\ncount_M = fraud_data[fraud_data['nameDest'].str.startswith('M')].shape[0]\ncount_C = fraud_data[fraud_data['nameDest'].str.startswith('C')].shape[0]\nprint(f\"Count of fraudulent transactions where nameDest is Merchant: {count_M}\")\nprint(f\"Count of fraudulent transactions where nameDest is customer: {count_C}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:49:08.385066Z","iopub.execute_input":"2024-09-28T11:49:08.385478Z","iopub.status.idle":"2024-09-28T11:49:08.417153Z","shell.execute_reply.started":"2024-09-28T11:49:08.385434Z","shell.execute_reply":"2024-09-28T11:49:08.415894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compare fraud cases across transaction types\nsns.countplot(x='type', hue='isFraud', data=data)\nplt.title('Fraudulent vs Non-Fraudulent Transactions by Type')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:49:08.419225Z","iopub.execute_input":"2024-09-28T11:49:08.419590Z","iopub.status.idle":"2024-09-28T11:49:17.070184Z","shell.execute_reply.started":"2024-09-28T11:49:08.419551Z","shell.execute_reply":"2024-09-28T11:49:17.068891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.5 Plot the number of transactions over time steps**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.lineplot(x='step', y='amount', hue='isFraud', data=data)\nplt.title('Transaction Amounts Over Time (Steps)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:49:17.071779Z","iopub.execute_input":"2024-09-28T11:49:17.072232Z","iopub.status.idle":"2024-09-28T11:50:48.193074Z","shell.execute_reply.started":"2024-09-28T11:49:17.072186Z","shell.execute_reply":"2024-09-28T11:50:48.191722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.6 Compare transaction amounts for fraud and non-fraud cases ,Use log scale to handle wide range of values**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.boxplot(x='isFraud', y='amount', data=data)\nplt.title('Transaction Amounts for Fraudulent vs Non-Fraudulent Transactions')\nplt.yscale('log')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:50:48.194656Z","iopub.execute_input":"2024-09-28T11:50:48.195081Z","iopub.status.idle":"2024-09-28T11:50:50.032457Z","shell.execute_reply.started":"2024-09-28T11:50:48.195019Z","shell.execute_reply":"2024-09-28T11:50:50.031249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.7 Number of Fraudulent Transactions Over Time**","metadata":{}},{"cell_type":"code","source":"data.groupby('step')['isFraud'].sum().plot()\nplt.title('Number of Fraudulent Transactions Over Time')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:50:50.033956Z","iopub.execute_input":"2024-09-28T11:50:50.034398Z","iopub.status.idle":"2024-09-28T11:50:50.514430Z","shell.execute_reply.started":"2024-09-28T11:50:50.034353Z","shell.execute_reply":"2024-09-28T11:50:50.512911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.8 Plot old balance origin vs. new balance origin**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.scatterplot(x='oldbalanceOrg', y='newbalanceOrig', hue='isFraud', data=data, alpha=0.5)\nplt.title('Old Balance vs New Balance (Origin) for Fraudulent vs Non-Fraudulent Transactions')\nplt.xscale('log')\nplt.yscale('log')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:50:50.516129Z","iopub.execute_input":"2024-09-28T11:50:50.516517Z","iopub.status.idle":"2024-09-28T11:53:48.420744Z","shell.execute_reply.started":"2024-09-28T11:50:50.516474Z","shell.execute_reply":"2024-09-28T11:53:48.419316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.9 Scatter plot of old balance vs. new balance for origin.**","metadata":{}},{"cell_type":"code","source":"##This scatter plot examines the relationship between the old balance (oldbalanceOrg) and the new balance (newbalanceOrig) for the origin account, with points colored based on fraud status (isFraud).\nsns.scatterplot(x='oldbalanceOrg', y='newbalanceOrig', hue='isFraud', data=data)\nplt.title('Old Balance vs. New Balance for Origin')\nplt.xlabel('Old Balance Origin')\nplt.ylabel('New Balance Origin')\nplt.show()\n##Relationship Insight: Helps identify if there's a particular pattern in the old vs. new balance related to fraud.\n##Fraud Detection: You can see if fraudulent transactions have distinctive patterns in balance changes.\n##If fraudulent transactions show specific patterns (larger changes in balance), this could help in designing better fraud detection mechanisms.\n\n# Scatter plot of old balance vs. new balance for destination\n##Similar to the previous scatter plot, this one explores the relationship between the old balance (oldbalanceDest) and new balance (newbalanceDest), but for the destination account instead of the origin.\nsns.scatterplot(x='oldbalanceDest', y='newbalanceDest', hue='isFraud', data=data)\nplt.title('Old Balance vs. New Balance for Destination')\nplt.xlabel('Old Balance Destination')\nplt.ylabel('New Balance Destination')\nplt.show()\n##Relationship Insight: Helps to understand how the transaction affects the destination balance and whether fraud is associated with specific balance changes.\n##Fraud Detection: Identifies if there are any noticeable patterns in balance changes for fraudulent transactions.\n## Help to find that fraudulent transactions to certain types of destination accounts have specific balance change characteristics, which could be indicative of fraud.\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T11:53:48.422404Z","iopub.execute_input":"2024-09-28T11:53:48.422828Z","iopub.status.idle":"2024-09-28T12:01:33.717104Z","shell.execute_reply.started":"2024-09-28T11:53:48.422783Z","shell.execute_reply":"2024-09-28T12:01:33.715729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.10 Percentage of Fraud by Transaction Type**","metadata":{}},{"cell_type":"code","source":"# Filter the DataFrame for fraudulent and non-fraudulent transactions\nfraud_data = data[data['isFraud'] == 1]\nnon_fraud_data = data[data['isFraud'] == 0]\n# Calculate the counts of different transaction types in fraudulent transactions\ntransaction_type_fraud_count = fraud_data['type'].value_counts()\n# Calculate the percentage of fraud for each transaction type\npercentage = (transaction_type_fraud_count / transaction_type_fraud_count.sum()) * 100\n# Print the percentage of fraud for each transaction type\nprint(percentage)\n\n# Plot the percentage of fraud for each transaction type\nplt.figure(figsize=(8, 6))\npercentage.plot(kind='bar')\n\nplt.xlabel('Transaction Type')\nplt.ylabel('Percentage')\nplt.title('Percentage of Fraud by Transaction Type')\nplt.xticks(rotation=45)\nplt.show()\n\n\n### So only 'cash out' and 'Transfer' have fradulant transaction. org has to focus in these two type.\n\n#Potential Issues:\n##Bias in Model Training: The model may learn to associate fraud only with 'CASH_OUT' and 'TRANSFER' transactions, potentially overlooking fraud in other types if they exist but are not represented in the training data.\n##Imbalanced Data: If fraudulent transactions are only present in certain types, the dataset could be highly imbalanced, which can lead to poor generalization and performance of the model.\n\n#How to address the problem\n#Collect More Data:\n##Expand Dataset: If possible, gather more data, especially for transaction types that currently have no fraudulent transactions. This can help ensure that the model is exposed to a more balanced distribution of transaction types.\n##Synthetic Data Generation: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic examples for transaction types with fewer instances.\n\n#Resampling Techniques:\n##Oversampling: Increase the number of instances of fraud in transaction types that currently have none by replicating or synthetically generating more samples.\n##Undersampling: Decrease the number of non-fraudulent samples in transaction types with many instances to balance the dataset.\n\n#Feature Engineering:\n##Include Transaction Type as a Feature: Incorporate the transaction type as a feature in the model, so the model learns to associate fraud with transaction types more explicitly.\n##Interaction Features: Create interaction features that combine transaction type with other features to better capture patterns specific to each type.\n\n#Model Evaluation:\n#Cross-Validation: Use techniques like k-fold cross-validation to ensure that the model’s performance is evaluated on different subsets of the data, helping to mitigate bias.\n#Class Weights: Adjust class weights in your model to give more importance to less frequent classes, which can help the model focus more on the underrepresented classes.\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:01:33.718816Z","iopub.execute_input":"2024-09-28T12:01:33.719253Z","iopub.status.idle":"2024-09-28T12:01:34.824769Z","shell.execute_reply.started":"2024-09-28T12:01:33.719208Z","shell.execute_reply":"2024-09-28T12:01:34.823499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['balance_diff_orig'] = data['oldbalanceOrg'] - data['newbalanceOrig']\ndata['balance_diff_dest'] = data['oldbalanceDest'] - data['newbalanceDest']\ndata['amount_to_oldbalance_ratio'] = (data['amount'] / data['oldbalanceOrg']) \ndata['amount_to_oldbalance_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Set NaN values in 'amount_to_oldbalance_ratio' to zero\ndata['amount_to_oldbalance_ratio'].fillna(0, inplace=True)\n\n## amount_to_oldbalance_ratio\n\n## High Ratio: A high amount_to_oldbalance_ratio may indicate unusually large transactions compared to the account's existing balance. \n## Such transactions could be suspicious and warrant further investigation.\n## Low Ratio: A very low ratio might indicate small transactions relative to the account balance, \n## which could be normal or expected behavior.\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:01:34.826371Z","iopub.execute_input":"2024-09-28T12:01:34.826770Z","iopub.status.idle":"2024-09-28T12:01:35.005783Z","shell.execute_reply.started":"2024-09-28T12:01:34.826726Z","shell.execute_reply":"2024-09-28T12:01:35.004500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.11  Label encoding**","metadata":{}},{"cell_type":"code","source":"objtlist = data.select_dtypes(include = \"object\").columns\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor i in objtlist:\n    data[i] = label_encoder.fit_transform(data[i].astype(str))\n    \nprint(data.info())\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:01:35.007129Z","iopub.execute_input":"2024-09-28T12:01:35.007502Z","iopub.status.idle":"2024-09-28T12:02:44.171680Z","shell.execute_reply.started":"2024-09-28T12:01:35.007461Z","shell.execute_reply":"2024-09-28T12:02:44.170434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.12 Handling missing Fraudalent transaction for Debit/Cash-in/Payment**\n##### 1. Use the Existing Dataset to Understand Correlations\n##### First, analyze the relationships between variables (e.g., amount, oldbalanceOrg, newbalanceOrig) across transaction types with observed fraud (CASH_OUT and TRANSFER). You can use statistical analysis, correlation matrices, or more sophisticated techniques like clustering","metadata":{}},{"cell_type":"code","source":"correlation = data.corr()\nprint(correlation[\"isFraud\"].sort_values(ascending=False))\n\n# Correlation matrix for CASH_OUT and TRANSFER transactions\nfraud_data = data[data['isFraud'] == 1]\ncorr_matrix = fraud_data[['amount', 'oldbalanceOrg', 'newbalanceOrig']].corr()\n\n# Plot correlation heatmap\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:02:44.173458Z","iopub.execute_input":"2024-09-28T12:02:44.173960Z","iopub.status.idle":"2024-09-28T12:02:48.636349Z","shell.execute_reply.started":"2024-09-28T12:02:44.173901Z","shell.execute_reply":"2024-09-28T12:02:48.635047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Cramér's V: Although not directly in the data generation, this measure informs you about the relevance of the categorical variables and their association with fraud.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import chi2_contingency\ndef calculate_cramers_v(df, cat_col, target_col):\n    # Create a contingency table\n    contingency_table = pd.crosstab(df[cat_col], df[target_col])\n    \n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Calculate Cramér's V\n    n = contingency_table.sum().sum()  # Total number of observations\n    r, k = contingency_table.shape  # Number of categories in each variable\n    cramers_v = np.sqrt(chi2 / (n * (min(k-1, r-1))))\n    \n    return cramers_v\n\n# Example for type and isFraud\ncramers_v_type_fraud = calculate_cramers_v(data, 'type', 'isFraud')\nprint(f\"Cramér's V for type and isFraud: {cramers_v_type_fraud}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:02:48.638517Z","iopub.execute_input":"2024-09-28T12:02:48.638956Z","iopub.status.idle":"2024-09-28T12:02:49.243606Z","shell.execute_reply.started":"2024-09-28T12:02:48.638912Z","shell.execute_reply":"2024-09-28T12:02:49.242298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Clustering: KMeans is applied to the fraudulent transactions from CASH_OUT and TRANSFER to identify patterns in fraud behavior.\n##### Synthetic Generation: For each cluster, the non-fraud transactions from DEBIT, CASH_IN, and PAYMENT are modified based on the fraud cluster's feature distributions, creating fraud-like synthetic transactions.\n##### Loop: The process is repeated for each transaction type (DEBIT, CASH_IN, PAYMENT) that doesn't have any fraudulent transactions.\n##### Correlation-based: Synthetic data is generated using the learned patterns from existing fraud cases, avoiding random data generation.\n##### Cluster-based: This ensures that the synthetic fraud data resembles the actual fraud behavior seen in CASH_OUT and TRANSFER.","metadata":{}},{"cell_type":"code","source":"# Assuming `non_fraud_data` is the dataset containing DEBIT, CASH_IN, and PAYMENT without fraud\n# and `fraud_data` contains CASH_OUT and TRANSFER with fraud\n\n# Step 1: Correlation analysis on CASH_OUT and TRANSFER\nfraud_only = fraud_data[fraud_data['isFraud'] == 1]\nfeatures = ['amount', 'oldbalanceOrg', 'newbalanceOrig']  # Add more numerical features if needed\n\n# Perform clustering on fraud cases\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nfraud_only['cluster'] = kmeans.fit_predict(fraud_only[features])\n\n# Analyze the feature distribution for each cluster\nfraud_clusters = fraud_only.groupby('cluster').mean(numeric_only=True)\n\n# Step 2: Use correlation from fraud clusters to generate synthetic fraud data\ndef generate_synthetic_data(non_fraud_subset, fraud_clusters, categorical_columns, n_samples=100):\n    \"\"\"Generate synthetic fraud data based on non-fraud and cluster distributions.\"\"\"\n    synthetic_data = []\n    for _, cluster in fraud_clusters.iterrows():\n        for _ in range(n_samples):\n            # Randomly sample from the non-fraud data to create a fraud-like example\n            sample = non_fraud_subset.sample(1).copy()\n            \n            # Adjust sample features based on cluster mean\n            for feature in features:\n                sample[feature] = np.random.normal(loc=cluster[feature], scale=0.1*cluster[feature])\n            \n            sample['isFraud'] = 1  # Label as fraud\n            \n            # Keep categorical columns unchanged\n            for cat_col in categorical_columns:\n                sample[cat_col] = non_fraud_subset[cat_col].iloc[0]\n                \n            synthetic_data.append(sample)\n    \n    return pd.concat(synthetic_data)\n\n# Categorical columns you want to preserve\ncategorical_columns = ['nameOrig', 'nameDest', 'type']  # Add other categorical columns if needed\n\n# Step 3: Apply the synthetic generation for each transaction type without fraud\nsynthetic_fraud_data = []\n\nfor transaction_type in ['DEBIT', 'CASH_IN', 'PAYMENT']:\n    print(f\"Generating synthetic fraud data for {transaction_type}\")\n    \n    # Filter non-fraud cases for the specific transaction type\n    non_fraud_subset = non_fraud_data[non_fraud_data['type'] == transaction_type]\n    \n    if non_fraud_subset.empty:\n        print(f\"No non-fraud data for {transaction_type}, skipping...\")\n        continue\n    \n    # Generate synthetic fraud data for the type based on fraud cluster correlations\n    synthetic_data = generate_synthetic_data(non_fraud_subset, fraud_clusters, categorical_columns, n_samples=500)\n    \n    # Add the type column back (though it should be retained via categorical columns)\n    synthetic_data['type'] = transaction_type\n    \n    synthetic_fraud_data.append(synthetic_data)\n\n# Combine the synthetic fraud data\nif synthetic_fraud_data:\n    synthetic_fraud_data_combined = pd.concat(synthetic_fraud_data, ignore_index=True)\n\n    # Add synthetic fraud data to the original dataset\n    complete_dataset = pd.concat([non_fraud_data, fraud_data, synthetic_fraud_data_combined], ignore_index=True)\n    \n    print(\"Synthetic fraud data generation complete!\")\nelse:\n    print(\"No synthetic fraud data was generated.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:02:49.245725Z","iopub.execute_input":"2024-09-28T12:02:49.246263Z","iopub.status.idle":"2024-09-28T12:06:20.819437Z","shell.execute_reply.started":"2024-09-28T12:02:49.246205Z","shell.execute_reply":"2024-09-28T12:06:20.818116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud_transactions = complete_dataset[complete_dataset['isFraud'] == 1]\nnon_fraud_transactions = complete_dataset[complete_dataset['isFraud'] == 0]\n\nprint(\"fraud_transactions\",fraud_transactions['type'].value_counts())\nprint(\"non_fraud_transactions\",non_fraud_transactions['type'].value_counts())\n\n# Step 1: Prepare the dataset (include both fraud and non-fraud cases)\nfraud_data = data[data['isFraud'] == 1]\nnon_fraud_data = data[data['isFraud'] == 0]","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:06:20.821302Z","iopub.execute_input":"2024-09-28T12:06:20.821731Z","iopub.status.idle":"2024-09-28T12:06:21.453947Z","shell.execute_reply.started":"2024-09-28T12:06:20.821685Z","shell.execute_reply":"2024-09-28T12:06:21.452719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Frequency encoding for 'nameOrig' and 'nameDest'\ndata['nameOrig_freq'] = data['nameOrig'].map(data['nameOrig'].value_counts())\ndata['nameDest_freq'] = data['nameDest'].map(data['nameDest'].value_counts())\n\n# Drop the original 'nameOrig' and 'nameDest' columns\ndata.drop(['nameOrig', 'nameDest'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:06:21.460516Z","iopub.execute_input":"2024-09-28T12:06:21.460963Z","iopub.status.idle":"2024-09-28T12:06:26.139834Z","shell.execute_reply.started":"2024-09-28T12:06:21.460917Z","shell.execute_reply":"2024-09-28T12:06:26.138560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.13 Calculating Balance Change Features**","metadata":{}},{"cell_type":"code","source":"#You want to track the decrease in the balance after a transaction and its relation to fraud. You can create new features to capture these insights:\n#Balance decrease ratio: Measure how much the balance has decreased after the transaction.\n#balance_decrease_ratio=amountoldbalanceOrg−newbalanceOrig\n#This shows how much of the original balance was spent.\n#Zero balance flag: A feature that flags if the oldbalanceOrg is zero, which could indicate suspicious activity for certain transaction types.\n\ndata['balance_decrease_ratio'] = (data['oldbalanceOrg'] - data['newbalanceOrig']) / data['amount']\ndata['is_old_balance_zero'] = data['oldbalanceOrg'] == 0\ndata['balance_decrease_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n# Set NaN values in 'amount_to_oldbalance_ratio' to zero\ndata['balance_decrease_ratio'].fillna(0, inplace=True)\n#TRANSFER and CASH_OUT transactions might be more likely to involve fraud\ndata['is_type_transfer'] = data['type'] == 'TRANSFER'\ndata['is_type_cash_out'] = data['type'] == 'CASH_OUT'\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:06:26.141635Z","iopub.execute_input":"2024-09-28T12:06:26.142146Z","iopub.status.idle":"2024-09-28T12:06:26.255865Z","shell.execute_reply.started":"2024-09-28T12:06:26.142088Z","shell.execute_reply":"2024-09-28T12:06:26.254776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.14 Scaling/Normalization & One-Hot Encoding**","metadata":{}},{"cell_type":"code","source":"# List of columns to scale/normalize\ncolumns_to_scale = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest','nameOrig_freq', 'nameDest_freq']\n\nif 'amount_to_oldbalance_ratio' in data.columns:\n    data.drop('amount_to_oldbalance_ratio', axis=1, inplace=True)\n    \n# Apply Standard scaling only to numerical columns\nscaler = StandardScaler()\ndata[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n\n# Ensure that after scaling, all columns (both scaled and categorical) are included in the dataset\ndf_scaled = data.copy()\n\n# Apply One-Hot Encoding on the 'type' column\ndf_scaled = pd.get_dummies(df_scaled, columns=['type'], drop_first=True)\n\n# Retain labels ('isFraud', 'isFlaggedFraud') as they are (not scaled/normalized)\nX = df_scaled.drop(['isFraud', 'isFlaggedFraud'], axis=1)  # Features\ny = df_scaled['isFraud']  # Target variable\n\n# Optional: Check the dataset after scaling and encoding\nprint(df_scaled.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:06:26.257447Z","iopub.execute_input":"2024-09-28T12:06:26.257865Z","iopub.status.idle":"2024-09-28T12:06:29.790617Z","shell.execute_reply.started":"2024-09-28T12:06:26.257821Z","shell.execute_reply":"2024-09-28T12:06:29.789440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3 Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"# **4. Model**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:06:29.792602Z","iopub.execute_input":"2024-09-28T12:06:29.793004Z","iopub.status.idle":"2024-09-28T12:06:42.806766Z","shell.execute_reply.started":"2024-09-28T12:06:29.792961Z","shell.execute_reply":"2024-09-28T12:06:42.805668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SMOT**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\n\n# Function to generate initial data and then use SMOTE for balancing\ndef generate_synthetic_data_with_smote(n_samples=10000, fraud_ratio=0.1):\n    # Generate the initial dataset using multivariate normal\n    mean = mean = [244, 179861, 833884, 855114, 1100702, 12249996, 0.00129082]  # example mean values for ['step', 'amount', 'oldbalanceOrg', etc.]\n    cov = [[1, 0.022, -0.01, -0.01, 0.028, 0.026, 0.032],  # correlation matrix from heatmap\n           [0.022, 1, -0.0028, -0.0079, 0.29, 0.46, 0.077],\n           [-0.01, -0.0028, 1, 1, 0.066, 0.042, 0.01],\n           [-0.01, -0.0079, 1, 1, 0.068, 0.042, -0.0081],\n           [0.028, 0.29, 0.066, 0.068, 1, 0.98, -0.0059],\n           [0.026, 0.46, 0.042, 0.042, 0.98, 1, 0.00054],\n           [0.032, 0.077, 0.01, -0.0081, -0.0059, 0.00054, 1]]\n    \n    synthetic_data = np.random.multivariate_normal(mean, cov, n_samples)\n\n    df = pd.DataFrame(synthetic_data, columns=['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFraud'])\n\n    # Convert 'isFraud' to binary 0 or 1 based on fraud_ratio\n    df['isFraud'] = np.where(df['isFraud'] > np.percentile(df['isFraud'], 100 * (1 - fraud_ratio)), 1, 0)\n\n    # Add synthetic 'type' data using a multinomial distribution based on correlation\n    type_probabilities = [0.15, 0.3, 0.05, 0.35, 0.15]  # probabilities for ['CASH_IN', 'CASH_OUT', 'DEBIT', 'PAYMENT', 'TRANSFER']\n    df['type'] = np.random.choice(['CASH_IN', 'CASH_OUT', 'DEBIT', 'PAYMENT', 'TRANSFER'], size=n_samples, p=type_probabilities)\n\n    # One-hot encoding of transaction types\n    df = pd.get_dummies(df, columns=['type'], drop_first=False)\n\n    # Split the features and target\n    X = df.drop('isFraud', axis=1)\n    y = df['isFraud']\n\n    # Standardizing the features before applying SMOTE\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Apply SMOTE to balance the classes\n    smote = SMOTE(sampling_strategy='auto', random_state=42)\n    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n\n    # Rebuild the dataframe after SMOTE\n    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n    df_resampled['isFraud'] = y_resampled\n\n    return df_resampled\n\n# Generate synthetic data with SMOTE\nsynthetic_data_smote = generate_synthetic_data_with_smote(n_samples=20000, fraud_ratio=0.2)\n\n# View the distribution\nprint(synthetic_data_smote['isFraud'].value_counts())  # Check class balance\n\n# Features and target variable\nX = synthetic_data_smote.drop(columns=['isFraud'])  # Features (all columns except 'isFraud')\ny = synthetic_data_smote['isFraud']  # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.0 Autoencoder**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\n\n# Build the AutoEncoder\ninput_dim = X_train_scaled.shape[1]\nencoding_dim = 16 # The size of the encoded representation\ninput_layer = Input(shape=(input_dim,))\nencoder = Dense(encoding_dim, activation='relu')(input_layer)\ndecoder = Dense(input_dim, activation='sigmoid')(encoder)\nautoencoder = Model(inputs=input_layer, outputs=decoder)\n# Compile the model\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')\n# Train the AutoEncoder\nautoencoder.fit(X_train_scaled, X_train_scaled, epochs=10, batch_size=32, shuffle=True, validation_split=0.2)\n# Extract the encoder part of the AutoEncoder\nencoder_model = Model(inputs=input_layer, outputs=encoder)\n# Encode the data (for downstream tasks)\nX_train_encoded = encoder_model.predict(X_train_scaled)\nX_test_encoded = encoder_model.predict(X_test_scaled)\n# Use the encoded data to build a Random Forest Classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_encoded, y_train)\n# Evaluate performance on the test set\ny_pred = rf_model.predict(X_test_encoded)\nprint(\"Sub-Experiment 1 - Direct Use of Pretrained Model\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n# Build a new model based on the pretrained encoder\nfor layer in encoder_model.layers:\nlayer.trainable = False # Freeze the pretrained encoder\n# Add new layers for classification on top of the frozen encoder\nencoded_input = encoder_model.output\nx = Dense(32, activation='relu')(encoded_input)\noutput = Dense(1, activation='sigmoid')(x)\nfine_tune_model = Model(inputs=encoder_model.input, outputs=output)\n# Compile and fine-tune the model\nfine_tune_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nfine_tune_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test,\ny_test))\n# Evaluate the model\ny_pred = (fine_tune_model.predict(X_test) > 0.5).astype(int)\nprint(\"Sub-Experiment 2 - Fine-Tuned Model\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n#Sub-Experiment 3: Fully Train a Model (No Pretraining)\n#Here, we train a model from scratch using the entire dataset (fully connected network).\n# Build a fully connected model from scratch\ninput_layer = Input(shape=(X_train_scaled.shape[1],))\nx = Dense(64, activation='relu')(input_layer)\nx = Dense(32, activation='relu')(x)\noutput_layer = Dense(1, activation='sigmoid')(x)\nfull_model = Model(inputs=input_layer, outputs=output_layer)\n# Compile and train the model\nfull_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nfull_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n# Evaluate the model\ny_pred = (full_model.predict(X_test) > 0.5).astype(int)\nprint(\"Sub-Experiment 3 - Fully Trained Model from Scratch\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T13:41:38.686983Z","iopub.execute_input":"2024-09-28T13:41:38.687607Z","iopub.status.idle":"2024-09-28T13:43:28.003901Z","shell.execute_reply.started":"2024-09-28T13:41:38.687554Z","shell.execute_reply":"2024-09-28T13:43:28.001828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SMOTE**","metadata":{}},{"cell_type":"code","source":"# Apply SMOTE only to the fraud class (minority class)\nsmote = SMOTE(sampling_strategy='minority', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\nX_resampled","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:50:11.521668Z","iopub.execute_input":"2024-09-28T12:50:11.522310Z","iopub.status.idle":"2024-09-28T12:50:21.685812Z","shell.execute_reply.started":"2024-09-28T12:50:11.522254Z","shell.execute_reply":"2024-09-28T12:50:21.684463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.1 Logistic Regression Model**","metadata":{}},{"cell_type":"code","source":"# Initialize Logistic Regression with class balancing\nlog_reg = LogisticRegression(class_weight='balanced',max_iter=500)\nlog_reg.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred_log_reg = log_reg.predict(X_test_scaled)\n\n# Evaluate Logistic Regression\nprint(\"Logistic Regression:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n\n# AUC-ROC Score\n# Get the probabilities for the positive class (fraud)\ny_proba_log_reg = log_reg.predict_proba(X_test_scaled)[:, 1]\nroc_score = roc_auc_score(y_test, y_proba_log_reg)\nprint(f\"AUC-ROC Score: {roc_score}\")\n\n# Plot ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_proba_log_reg)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_score:.2f})\")\nplt.plot([0, 1], [0, 1], color='grey', linestyle='--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic (ROC) Curve\")\nplt.legend()\nplt.show()\n\n# Cross-Validation\ncv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='roc_auc')\nprint(f\"Cross-Validation AUC-ROC Scores: {cv_scores}\")\nprint(f\"Mean Cross-Validation AUC-ROC Score: {cv_scores.mean()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:50:05.017935Z","iopub.status.idle":"2024-09-28T12:50:05.018478Z","shell.execute_reply.started":"2024-09-28T12:50:05.018212Z","shell.execute_reply":"2024-09-28T12:50:05.018235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.2 Decision Tree Model**","metadata":{}},{"cell_type":"code","source":"# Initialize Decision Tree with class balancing\ndecision_tree = DecisionTreeClassifier(class_weight='balanced', max_depth=10, random_state=42)\ndecision_tree.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred_tree = decision_tree.predict(X_test_scaled)\n\n# Evaluate Decision Tree\nprint(\"Decision Tree:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_tree))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_tree))\n\n# AUC-ROC Score\ny_proba_tree = decision_tree.predict_proba(X_test_scaled)[:, 1]\nroc_score_tree = roc_auc_score(y_test, y_proba_tree)\nprint(f\"AUC-ROC Score: {roc_score_tree}\")\n\n# Plot ROC curve\nfpr_tree, tpr_tree, thresholds_tree = roc_curve(y_test, y_proba_tree)\nplt.figure(figsize=(8,6))\nplt.plot(fpr_tree, tpr_tree, color='green', label=f\"ROC Curve (AUC = {roc_score_tree:.2f})\")\nplt.plot([0, 1], [0, 1], color='grey', linestyle='--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Decision Tree: Receiver Operating Characteristic (ROC) Curve\")\nplt.legend()\nplt.show()\n\n# Cross-Validation\ncv_scores_tree = cross_val_score(decision_tree, X_train_scaled, y_train, cv=5, scoring='roc_auc')\nprint(f\"Cross-Validation AUC-ROC Scores: {cv_scores_tree}\")\nprint(f\"Mean Cross-Validation AUC-ROC Score: {cv_scores_tree.mean()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:50:05.020367Z","iopub.status.idle":"2024-09-28T12:50:05.021008Z","shell.execute_reply.started":"2024-09-28T12:50:05.020684Z","shell.execute_reply":"2024-09-28T12:50:05.020715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.3 Deep Neural Network Architecture**","metadata":{}},{"cell_type":"code","source":"# Function to build the model\ndef build_fraud_detection_model(input_shape):\n    model = models.Sequential()\n    \n    # First layer: Input layer\n    model.add(layers.Input(shape=input_shape))  # Define input shape using Input layer\n    \n    model.add(layers.Dense(512, activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Dense(128, activation='relu'))\n    model.add(layers.BatchNormalization())\n    \n    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# Number of random splits for validation\nn_splits = 5\ncv_scores = []\n\n# Train-Validate Split\ninput_shape = (X_train_scaled.shape[1],)  # Input shape of the features\n\nfor i in range(n_splits):\n    print(f\"\\nStarting fold {i+1}/{n_splits}\")\n    \n    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(\n        X_train_scaled, y_train, test_size=0.2, stratify=y_train, random_state=i)\n    \n    model = build_fraud_detection_model(input_shape)  # New model instance for each split\n    print(f\"Training on fold {i+1}...\")\n    \n    history = model.fit(X_train_fold, y_train_fold, epochs=1, batch_size=64, validation_data=(X_val_fold, y_val_fold), verbose=0)\n    \n    # Predict probabilities for the AUC-ROC\n    y_val_pred_proba = model.predict(X_val_fold)\n    \n    # Calculate ROC AUC score\n    auc_score = roc_auc_score(y_val_fold, y_val_pred_proba)\n    cv_scores.append(auc_score)\n    \n    # Print the results for this fold\n    print(f\"Fold {i+1} AUC-ROC Score: {auc_score:.4f}\")\n    print(f\"Fold {i+1} Training Loss: {history.history['loss'][-1]:.4f}, Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n\n# Final cross-validation AUC scores\nprint(f\"\\nCross-Validation AUC Scores: {cv_scores}\")\nprint(f\"Mean Cross-Validation AUC Score: {np.mean(cv_scores):.4f}\")\n\n# Final evaluation on the test set\nprint(\"\\nTraining the final model on the full dataset...\")\nmodel_final = build_fraud_detection_model(input_shape)\nhistory_final = model_final.fit(X_train_scaled, y_train, epochs=1, batch_size=64, validation_split=0.2, verbose=1)\n\nloss, accuracy = model_final.evaluate(X_test_scaled, y_test)\nprint(f\"Test Accuracy: {accuracy}\")\n\n# Predict probabilities for the AUC-ROC\ny_pred_proba_final = model_final.predict(X_test_scaled)\n\n# ROC Curve and AUC Score\nroc_score = roc_auc_score(y_test, y_pred_proba_final)\nprint(f\"AUC-ROC Score: {roc_score:.4f}\")\n\n# Plot ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba_final)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"AUC = {roc_score:.2f}\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:50:05.022849Z","iopub.status.idle":"2024-09-28T12:50:05.023502Z","shell.execute_reply.started":"2024-09-28T12:50:05.023173Z","shell.execute_reply":"2024-09-28T12:50:05.023208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **4.4 Evaluating Top 5 Hugging Face Models – DeiT, ResNet-50, ViT, Swin Transformer, and ConvNeXT**","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model_name, model_path, X_test, y_test):\n    try:\n        # Load the model and tokenizer from the local path\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n        # Tokenize the input\n        inputs = tokenizer(list(X_test), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n\n        # Make predictions\n        with torch.no_grad():\n            logits = model(**inputs).logits\n        y_pred_proba = torch.sigmoid(logits).numpy()[:, 1]  # Probability for the positive class\n        y_pred = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary predictions\n\n        # Calculate metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        conf_matrix = confusion_matrix(y_test, y_pred)\n        class_report = classification_report(y_test, y_pred)\n        roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n        # Print results\n        print(f\"Model: {model_name}\")\n        print(\"Accuracy:\", accuracy)\n        print(\"Confusion Matrix:\\n\", conf_matrix)\n        print(\"Classification Report:\\n\", class_report)\n        print(f\"AUC-ROC Score: {roc_auc}\")\n\n        # Plot ROC Curve\n        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n        plt.figure(figsize=(8, 6))\n        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n        plt.plot([0, 1], [0, 1], linestyle=\"--\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.title(f\"ROC Curve - {model_name}\")\n        plt.legend()\n        plt.show()\n\n    except Exception as e:\n        print(f\"An error occurred while evaluating {model_name}: {str(e)}\")\n\n# List of top 5 classification models with local paths on Kaggle\nmodels = [\n    {\"name\": \"facebook/deit-base-distilled-patch16-224\", \"path\": \"/kaggle/input/models/deit-base-distilled-patch16-224\"},\n    {\"name\": \"microsoft/resnet-50\", \"path\": \"/kaggle/input/models/resnet-50\"},\n    {\"name\": \"google/vit-base-patch16-224\", \"path\": \"/kaggle/input/models/vit-base-patch16-224\"},\n    {\"name\": \"microsoft/swin-base-patch4-window7-224\", \"path\": \"/kaggle/input/models/swin-base-patch4-window7-224\"},\n    {\"name\": \"facebook/convnext-base-224\", \"path\": \"/kaggle/input/models/convnext-base-224\"}\n]\n\n# Evaluate each model\nfor model in models:\n    evaluate_model(model['name'], model['path'], X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:50:05.026852Z","iopub.status.idle":"2024-09-28T12:50:05.027373Z","shell.execute_reply.started":"2024-09-28T12:50:05.027141Z","shell.execute_reply":"2024-09-28T12:50:05.027166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.5 Comparative Analysis of Pre-Trained Classification Models**","metadata":{}},{"cell_type":"code","source":"models = {\n    \"Support Vector Machine\": SVC(probability=True),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n    \"Naive Bayes\": GaussianNB(),\n    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n}\n\n# Iterate over models\nfor model_name, model in models.items():\n    # Fit the model (for demonstration; typically, you load a pre-trained model)\n    model.fit(X_test_scaled, y_test)  # Use pre-trained model loading here if applicable\n\n    # Make predictions\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Get probabilities for the positive class\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    conf_matrix = confusion_matrix(y_test, y_pred)\n    class_report = classification_report(y_test, y_pred)\n    auc_roc = roc_auc_score(y_test, y_pred_proba)\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n\n    # Print results\n    print(f\"\\nModel: {model_name}\")\n    print(\"Accuracy:\", accuracy)\n    print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n    print(\"\\nClassification Report:\\n\", class_report)\n    print(\"AUC-ROC Score:\", auc_roc)\n\n    # Plotting False Positive Curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(fpr, tpr, color='blue', label='ROC Curve')\n    plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n    plt.title(f'ROC Curve - {model_name}')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T13:44:49.575616Z","iopub.execute_input":"2024-09-28T13:44:49.576273Z"},"trusted":true},"execution_count":null,"outputs":[]}]}