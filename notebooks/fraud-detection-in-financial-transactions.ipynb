{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**:  \n",
    "Develop a machine learning model to detect fraudulent transactions in real-time, enhancing the security of financial systems.\n",
    "\n",
    "**Introduction**:  \n",
    "Fraudulent financial transactions can lead to significant losses. Detecting fraud in real-time can enhance the security of financial systems.\n",
    "\n",
    "**Relevance**:  \n",
    "Detecting fraud in real-time is essential for financial institutions to protect their customers and assets.\n",
    "\n",
    "**Data Source**:  \n",
    "Financial Transactions Dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load all the required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:30.849160Z",
     "iopub.status.busy": "2024-10-08T20:08:30.848212Z",
     "iopub.status.idle": "2024-10-08T20:08:30.861067Z",
     "shell.execute_reply": "2024-10-08T20:08:30.859873Z",
     "shell.execute_reply.started": "2024-10-08T20:08:30.849100Z"
    }
   },
   "outputs": [],
   "source": [
    "# General-purpose libraries\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, \n",
    "                             classification_report, roc_auc_score, \n",
    "                             roc_curve)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Handling imbalanced datasets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning libraries\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Hugging Face transformers for pre-trained models and tokenizers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Utility libraries\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:30.894609Z",
     "iopub.status.busy": "2024-10-08T20:08:30.893851Z",
     "iopub.status.idle": "2024-10-08T20:08:46.810311Z",
     "shell.execute_reply": "2024-10-08T20:08:46.809284Z",
     "shell.execute_reply.started": "2024-10-08T20:08:30.894565Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/financial-dataset/Synthetic_Financial_datasets_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transaction Dataset Field Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:46.812772Z",
     "iopub.status.busy": "2024-10-08T20:08:46.812372Z",
     "iopub.status.idle": "2024-10-08T20:08:46.830195Z",
     "shell.execute_reply": "2024-10-08T20:08:46.828970Z",
     "shell.execute_reply.started": "2024-10-08T20:08:46.812730Z"
    }
   },
   "outputs": [],
   "source": [
    "fields = {\n",
    "    \"Step\": \"Represents a unit of time in hours. The simulation spans 744 steps (equivalent to 31 days).\",\n",
    "    \"type\": \"The type of transaction, which includes categories like CASH-IN, CASH-OUT, DEBIT, PAYMENT, and TRANSFER.\",\n",
    "    \"amount\": \"The transaction amount in the local currency.\",\n",
    "    \"nameOrig\": \"The ID of the customer initiating the transaction.\",\n",
    "    \"oldbalanceOrg\": \"The balance of the customer before the transaction.\",\n",
    "    \"newbalanceOrig\": \"The balance of the customer after the transaction.\",\n",
    "    \"nameDest\": \"The ID of the recipient of the transaction.\",\n",
    "    \"oldbalanceDest\": \"The balance of the recipient before the transaction.\",\n",
    "    \"newbalanceDest\": \"The balance of the recipient after the transaction.\",\n",
    "    \"isFraud\": \"A binary flag indicating whether the transaction is fraudulent.\",\n",
    "    \"isFlaggedFraud\": \"A binary flag indicating whether the transaction was flagged as potentially fraudulent.\"\n",
    "}\n",
    "\n",
    "for i, (field, description) in enumerate(fields.items(), start=1):\n",
    "    print(f\"{i}. {field}: {description}\")\n",
    "\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:46.831909Z",
     "iopub.status.busy": "2024-10-08T20:08:46.831566Z",
     "iopub.status.idle": "2024-10-08T20:08:48.577774Z",
     "shell.execute_reply": "2024-10-08T20:08:48.576711Z",
     "shell.execute_reply.started": "2024-10-08T20:08:46.831848Z"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:48.580624Z",
     "iopub.status.busy": "2024-10-08T20:08:48.580291Z",
     "iopub.status.idle": "2024-10-08T20:08:49.524138Z",
     "shell.execute_reply": "2024-10-08T20:08:49.523065Z",
     "shell.execute_reply.started": "2024-10-08T20:08:48.580588Z"
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:49.525746Z",
     "iopub.status.busy": "2024-10-08T20:08:49.525384Z",
     "iopub.status.idle": "2024-10-08T20:08:49.542378Z",
     "shell.execute_reply": "2024-10-08T20:08:49.541267Z",
     "shell.execute_reply.started": "2024-10-08T20:08:49.525708Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:49.544453Z",
     "iopub.status.busy": "2024-10-08T20:08:49.543997Z",
     "iopub.status.idle": "2024-10-08T20:08:50.170027Z",
     "shell.execute_reply": "2024-10-08T20:08:50.168722Z",
     "shell.execute_reply.started": "2024-10-08T20:08:49.544398Z"
    }
   },
   "outputs": [],
   "source": [
    "data['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:50.171842Z",
     "iopub.status.busy": "2024-10-08T20:08:50.171465Z",
     "iopub.status.idle": "2024-10-08T20:08:50.245009Z",
     "shell.execute_reply": "2024-10-08T20:08:50.243610Z",
     "shell.execute_reply.started": "2024-10-08T20:08:50.171803Z"
    }
   },
   "outputs": [],
   "source": [
    "data['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2) Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.0 Distribution of Transaction Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:50.247681Z",
     "iopub.status.busy": "2024-10-08T20:08:50.246714Z",
     "iopub.status.idle": "2024-10-08T20:08:58.543378Z",
     "shell.execute_reply": "2024-10-08T20:08:58.542237Z",
     "shell.execute_reply.started": "2024-10-08T20:08:50.247637Z"
    }
   },
   "outputs": [],
   "source": [
    "transaction_examples = {\n",
    "    \"CASH-IN\": [\"ATM Deposit\",\"Cash Deposit at Bank\",\"Check Deposit\"],\n",
    "    \"CASH-OUT\": [\"ATM Withdrawal\",\"Credit Card Cash Advance\",\"Withdrawal from Investment Account\"],\n",
    "    \"DEBIT\": [\"Retail Purchase\",\"Online Shopping\",\"Restaurant Payment\"],\n",
    "    \"PAYMENT\": [\"Utility Bill Payment\",\"Credit Card Bill Payment\",\"Subscription Payment\"],\n",
    "    \"TRANSFER\": [\"Internal Transfer\",\"External Transfer\", \"Peer-to-Peer Transfer\"]\n",
    "}\n",
    "\n",
    "for i, (transaction_type, examples) in enumerate(transaction_examples.items(), start=1):\n",
    "    print(f\"{i}. {transaction_type}:\")\n",
    "    for j, example in enumerate(examples, start=1):\n",
    "        print(f\"   {i}.{j} {example}\")\n",
    "\n",
    "sns.countplot(x='type', data=data)\n",
    "plt.title('Distribution of Transaction Types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.1 Analysis of Transaction Distribution During Business and Non-Business Hours: Insights on Fraudulent vs Non-Fraudulent Transactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:08:58.545084Z",
     "iopub.status.busy": "2024-10-08T20:08:58.544725Z",
     "iopub.status.idle": "2024-10-08T20:09:06.745830Z",
     "shell.execute_reply": "2024-10-08T20:09:06.744639Z",
     "shell.execute_reply.started": "2024-10-08T20:08:58.545047Z"
    }
   },
   "outputs": [],
   "source": [
    "data['hour_of_day'] = data['step'] % 24\n",
    "data['day'] = data['step'] // 24\n",
    "\n",
    "# Create a new column to differentiate business hours (10 AM to 6 PM)\n",
    "def categorize_business_hour(hour):\n",
    "    if 10 <= hour < 18:\n",
    "        return 'Business Hour (10AM-6PM)'\n",
    "    else:\n",
    "        return 'Non-Business Hour'\n",
    "\n",
    "data['time_category'] = data['hour_of_day'].apply(categorize_business_hour)\n",
    "#Analyze the number of transactions in business and non-business hours\n",
    "transaction_counts = data['time_category'].value_counts()\n",
    "\n",
    "#Analyze the number of fraudulent transactions in business and non-business hours\n",
    "fraud_counts = data[data['isFraud'] == 1]['time_category'].value_counts()\n",
    "\n",
    "#Plot the distribution of transactions during business and non-business hours\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=data, x='time_category', hue='isFraud', palette='Set2')\n",
    "plt.title('Transaction Count: Business vs Non-Business Hours (Fraud vs Non-Fraud)')\n",
    "plt.xlabel('Time Category')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.legend(title='Fraud', loc='upper right', labels=['Non-Fraudulent', 'Fraudulent'])\n",
    "plt.show()\n",
    "\n",
    "# Display the counts for both overall transactions and fraudulent transactions\n",
    "print(\"Overall Transactions Count:\")\n",
    "print(transaction_counts)\n",
    "\n",
    "print(\"\\nFraudulent Transactions Count:\")\n",
    "print(fraud_counts)\n",
    "\n",
    "data.drop(columns=['hour_of_day', 'day', 'time_category'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.2 Fraud and non-fraud transaction type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:09:06.751489Z",
     "iopub.status.busy": "2024-10-08T20:09:06.751036Z",
     "iopub.status.idle": "2024-10-08T20:09:08.505066Z",
     "shell.execute_reply": "2024-10-08T20:09:08.504028Z",
     "shell.execute_reply.started": "2024-10-08T20:09:06.751451Z"
    }
   },
   "outputs": [],
   "source": [
    "fraud_transactions = data[data['isFraud'] == 1]\n",
    "non_fraud_transactions = data[data['isFraud'] == 0]\n",
    "\n",
    "print(\"fraud_transactions\",fraud_transactions['type'].value_counts())\n",
    "print(\"non_fraud_transactions\",non_fraud_transactions['type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3 Count of fraudulent transactions based on Merchant and Customer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:09:08.506581Z",
     "iopub.status.busy": "2024-10-08T20:09:08.506269Z",
     "iopub.status.idle": "2024-10-08T20:09:08.531817Z",
     "shell.execute_reply": "2024-10-08T20:09:08.530750Z",
     "shell.execute_reply.started": "2024-10-08T20:09:08.506547Z"
    }
   },
   "outputs": [],
   "source": [
    "fraud_data = data[data['isFraud'] == 1]\n",
    "count_M = fraud_data[fraud_data['nameDest'].str.startswith('M')].shape[0]\n",
    "count_C = fraud_data[fraud_data['nameDest'].str.startswith('C')].shape[0]\n",
    "print(f\"Count of fraudulent transactions where nameDest is Merchant: {count_M}\")\n",
    "print(f\"Count of fraudulent transactions where nameDest is customer: {count_C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:09:08.534214Z",
     "iopub.status.busy": "2024-10-08T20:09:08.533275Z",
     "iopub.status.idle": "2024-10-08T20:09:13.513222Z",
     "shell.execute_reply": "2024-10-08T20:09:13.512099Z",
     "shell.execute_reply.started": "2024-10-08T20:09:08.534160Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compare fraud cases across transaction types\n",
    "sns.countplot(x='type', hue='isFraud', data=data)\n",
    "plt.title('Fraudulent vs Non-Fraudulent Transactions by Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4 Plot the number of transactions over time steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:09:13.515284Z",
     "iopub.status.busy": "2024-10-08T20:09:13.514801Z",
     "iopub.status.idle": "2024-10-08T20:09:13.899459Z",
     "shell.execute_reply": "2024-10-08T20:09:13.898341Z",
     "shell.execute_reply.started": "2024-10-08T20:09:13.515233Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_per_step = data.groupby('step').size()\n",
    "transactions_per_step.plot(kind='line', title='Transaction Volume Over Time')\n",
    "#insights: Identifying the peaks in transaction activity. Certain hours with more activity or spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5 Compare transaction amounts for fraud and non-fraud cases ,Use log scale to handle wide range of values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:09:13.901291Z",
     "iopub.status.busy": "2024-10-08T20:09:13.900918Z",
     "iopub.status.idle": "2024-10-08T20:09:15.640323Z",
     "shell.execute_reply": "2024-10-08T20:09:15.638824Z",
     "shell.execute_reply.started": "2024-10-08T20:09:13.901251Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='isFraud', y='amount', data=data)\n",
    "plt.title('Transaction Amounts for Fraudulent vs Non-Fraudulent Transactions')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6 Plot old balance origin vs. new balance origin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:09:15.642069Z",
     "iopub.status.busy": "2024-10-08T20:09:15.641706Z",
     "iopub.status.idle": "2024-10-08T20:11:31.063579Z",
     "shell.execute_reply": "2024-10-08T20:11:31.062404Z",
     "shell.execute_reply.started": "2024-10-08T20:09:15.642031Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='oldbalanceOrg', y='newbalanceOrig', hue='isFraud', data=data, alpha=0.5)\n",
    "plt.title('Old Balance vs New Balance (Origin) for Fraudulent vs Non-Fraudulent Transactions')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7 Scatter plot of old balance vs. new balance for origin.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:11:31.065477Z",
     "iopub.status.busy": "2024-10-08T20:11:31.065108Z",
     "iopub.status.idle": "2024-10-08T20:17:41.095915Z",
     "shell.execute_reply": "2024-10-08T20:17:41.094584Z",
     "shell.execute_reply.started": "2024-10-08T20:11:31.065438Z"
    }
   },
   "outputs": [],
   "source": [
    "##This scatter plot examines the relationship between the old balance (oldbalanceOrg) and the new balance (newbalanceOrig) for the origin account, with points colored based on fraud status (isFraud).\n",
    "sns.scatterplot(x='oldbalanceOrg', y='newbalanceOrig', hue='isFraud', data=data)\n",
    "plt.title('Old Balance vs. New Balance for Origin')\n",
    "plt.xlabel('Old Balance Origin')\n",
    "plt.ylabel('New Balance Origin')\n",
    "plt.show()\n",
    "##Relationship Insight: Helps identify if there's a particular pattern in the old vs. new balance related to fraud.\n",
    "##Fraud Detection: You can see if fraudulent transactions have distinctive patterns in balance changes.\n",
    "##If fraudulent transactions show specific patterns (larger changes in balance), this could help in designing better fraud detection mechanisms.\n",
    "\n",
    "# Scatter plot of old balance vs. new balance for destination\n",
    "##Similar to the previous scatter plot, this one explores the relationship between the old balance (oldbalanceDest) and new balance (newbalanceDest), but for the destination account instead of the origin.\n",
    "sns.scatterplot(x='oldbalanceDest', y='newbalanceDest', hue='isFraud', data=data)\n",
    "plt.title('Old Balance vs. New Balance for Destination')\n",
    "plt.xlabel('Old Balance Destination')\n",
    "plt.ylabel('New Balance Destination')\n",
    "plt.show()\n",
    "##Relationship Insight: Helps to understand how the transaction affects the destination balance and whether fraud is associated with specific balance changes.\n",
    "##Fraud Detection: Identifies if there are any noticeable patterns in balance changes for fraudulent transactions.\n",
    "## Help to find that fraudulent transactions to certain types of destination accounts have specific balance change characteristics, which could be indicative of fraud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.8 Percentage of Fraud by Transaction Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:17:41.097998Z",
     "iopub.status.busy": "2024-10-08T20:17:41.097536Z",
     "iopub.status.idle": "2024-10-08T20:17:42.203186Z",
     "shell.execute_reply": "2024-10-08T20:17:42.202020Z",
     "shell.execute_reply.started": "2024-10-08T20:17:41.097948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for fraudulent and non-fraudulent transactions\n",
    "fraud_data = data[data['isFraud'] == 1]\n",
    "non_fraud_data = data[data['isFraud'] == 0]\n",
    "# Calculate the counts of different transaction types in fraudulent transactions\n",
    "transaction_type_fraud_count = fraud_data['type'].value_counts()\n",
    "# Calculate the percentage of fraud for each transaction type\n",
    "percentage = (transaction_type_fraud_count / transaction_type_fraud_count.sum()) * 100\n",
    "# Print the percentage of fraud for each transaction type\n",
    "print(percentage)\n",
    "\n",
    "# Plot the percentage of fraud for each transaction type\n",
    "plt.figure(figsize=(8, 6))\n",
    "percentage.plot(kind='bar')\n",
    "\n",
    "plt.xlabel('Transaction Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage of Fraud by Transaction Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### So only 'cash out' and 'Transfer' have fradulant transaction. org has to focus in these two type.\n",
    "\n",
    "#Potential Issues:\n",
    "##Bias in Model Training: The model may learn to associate fraud only with 'CASH_OUT' and 'TRANSFER' transactions, potentially overlooking fraud in other types if they exist but are not represented in the training data.\n",
    "##Imbalanced Data: If fraudulent transactions are only present in certain types, the dataset could be highly imbalanced, which can lead to poor generalization and performance of the model.\n",
    "\n",
    "#How to address the problem\n",
    "#Collect More Data:\n",
    "##Expand Dataset: If possible, gather more data, especially for transaction types that currently have no fraudulent transactions. This can help ensure that the model is exposed to a more balanced distribution of transaction types.\n",
    "##Synthetic Data Generation: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic examples for transaction types with fewer instances.\n",
    "\n",
    "#Resampling Techniques:\n",
    "##Oversampling: Increase the number of instances of fraud in transaction types that currently have none by replicating or synthetically generating more samples.\n",
    "##Undersampling: Decrease the number of non-fraudulent samples in transaction types with many instances to balance the dataset.\n",
    "\n",
    "#Feature Engineering:\n",
    "##Include Transaction Type as a Feature: Incorporate the transaction type as a feature in the model, so the model learns to associate fraud with transaction types more explicitly.\n",
    "##Interaction Features: Create interaction features that combine transaction type with other features to better capture patterns specific to each type.\n",
    "\n",
    "#Model Evaluation:\n",
    "#Cross-Validation: Use techniques like k-fold cross-validation to ensure that the model’s performance is evaluated on different subsets of the data, helping to mitigate bias.\n",
    "#Class Weights: Adjust class weights in your model to give more importance to less frequent classes, which can help the model focus more on the underrepresented classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.9  Label encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:17:42.204937Z",
     "iopub.status.busy": "2024-10-08T20:17:42.204532Z",
     "iopub.status.idle": "2024-10-08T20:17:43.742566Z",
     "shell.execute_reply": "2024-10-08T20:17:43.741483Z",
     "shell.execute_reply.started": "2024-10-08T20:17:42.204894Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-Hot Encode the 'Type' column to include it in correlation\n",
    "data_encoded = pd.get_dummies(data, columns=['type'], drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.10 Handling missing Fraudalent transaction for Debit/Cash-in/Payment**\n",
    "##### 1. Use the Existing Dataset to Understand Correlations\n",
    "##### First, analyze the relationships between variables (e.g., amount, oldbalanceOrg, newbalanceOrig) across transaction types with observed fraud (CASH_OUT and TRANSFER). You can use statistical analysis, correlation matrices, or more sophisticated techniques like clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Method-1 - K-means Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:17:43.744604Z",
     "iopub.status.busy": "2024-10-08T20:17:43.744152Z",
     "iopub.status.idle": "2024-10-08T20:18:41.506014Z",
     "shell.execute_reply": "2024-10-08T20:18:41.504933Z",
     "shell.execute_reply.started": "2024-10-08T20:17:43.744550Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "objtlist = data.select_dtypes(include = \"object\").columns\n",
    "label_encoder = LabelEncoder()\n",
    " \n",
    "for i in objtlist:\n",
    "    data[i] = label_encoder.fit_transform(data[i].astype(str))\n",
    "\n",
    "correlation = data.corr()\n",
    "print(correlation[\"isFraud\"].sort_values(ascending=False))\n",
    "\n",
    "# Correlation matrix for CASH_OUT and TRANSFER transactions\n",
    "fraud_data = data[data['isFraud'] == 1]\n",
    "corr_matrix = fraud_data[['amount', 'oldbalanceOrg', 'newbalanceOrig']].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cramér's V: Although not directly in the data generation, this measure informs you about the relevance of the categorical variables and their association with fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:18:41.508285Z",
     "iopub.status.busy": "2024-10-08T20:18:41.507697Z",
     "iopub.status.idle": "2024-10-08T20:18:42.110644Z",
     "shell.execute_reply": "2024-10-08T20:18:42.109461Z",
     "shell.execute_reply.started": "2024-10-08T20:18:41.508232Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_cramers_v(df, cat_col, target_col):\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(df[cat_col], df[target_col])\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Calculate Cramér's V\n",
    "    n = contingency_table.sum().sum()  # Total number of observations\n",
    "    r, k = contingency_table.shape  # Number of categories in each variable\n",
    "    cramers_v = np.sqrt(chi2 / (n * (min(k-1, r-1))))\n",
    "    \n",
    "    return cramers_v\n",
    "\n",
    "# Example for type and isFraud\n",
    "cramers_v_type_fraud = calculate_cramers_v(data, 'type', 'isFraud')\n",
    "print(f\"Cramér's V for type and isFraud: {cramers_v_type_fraud}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clustering**: KMeans is applied to the fraudulent transactions from CASH_OUT and TRANSFER to identify patterns in fraud behavior.\n",
    "### Synthetic Generation: For each cluster, the non-fraud transactions from DEBIT, CASH_IN, and PAYMENT are modified based on the fraud cluster's feature distributions, creating fraud-like synthetic transactions.\n",
    "### Loop: The process is repeated for each transaction type (DEBIT, CASH_IN, PAYMENT) that doesn't have any fraudulent transactions.\n",
    "### Correlation-based: Synthetic data is generated using the learned patterns from existing fraud cases, avoiding random data generation.\n",
    "### Cluster-based: This ensures that the synthetic fraud data resembles the actual fraud behavior seen in CASH_OUT and TRANSFER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:18:42.113082Z",
     "iopub.status.busy": "2024-10-08T20:18:42.112611Z",
     "iopub.status.idle": "2024-10-08T20:21:42.297321Z",
     "shell.execute_reply": "2024-10-08T20:21:42.295968Z",
     "shell.execute_reply.started": "2024-10-08T20:18:42.113029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming `non_fraud_data` is the dataset containing DEBIT, CASH_IN, and PAYMENT without fraud\n",
    "# and `fraud_data` contains CASH_OUT and TRANSFER with fraud\n",
    "\n",
    "# Step 1: Correlation analysis on CASH_OUT and TRANSFER\n",
    "fraud_only = fraud_data[fraud_data['isFraud'] == 1]\n",
    "features = ['amount', 'oldbalanceOrg', 'newbalanceOrig']  # Add more numerical features if needed\n",
    "\n",
    "# Perform clustering on fraud cases\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "fraud_only['cluster'] = kmeans.fit_predict(fraud_only[features])\n",
    "\n",
    "# Analyze the feature distribution for each cluster\n",
    "fraud_clusters = fraud_only.groupby('cluster').mean(numeric_only=True)\n",
    "\n",
    "# Step 2: Use correlation from fraud clusters to generate synthetic fraud data\n",
    "def generate_synthetic_data(non_fraud_subset, fraud_clusters, categorical_columns, n_samples=100):\n",
    "    \"\"\"Generate synthetic fraud data based on non-fraud and cluster distributions.\"\"\"\n",
    "    synthetic_data = []\n",
    "    for _, cluster in fraud_clusters.iterrows():\n",
    "        for _ in range(n_samples):\n",
    "            # Randomly sample from the non-fraud data to create a fraud-like example\n",
    "            sample = non_fraud_subset.sample(1).copy()\n",
    "            \n",
    "            # Adjust sample features based on cluster mean\n",
    "            for feature in features:\n",
    "                sample[feature] = np.random.normal(loc=cluster[feature], scale=0.1*cluster[feature])\n",
    "            \n",
    "            sample['isFraud'] = 1  # Label as fraud\n",
    "            \n",
    "            # Keep categorical columns unchanged\n",
    "            for cat_col in categorical_columns:\n",
    "                sample[cat_col] = non_fraud_subset[cat_col].iloc[0]\n",
    "                \n",
    "            synthetic_data.append(sample)\n",
    "    \n",
    "    return pd.concat(synthetic_data)\n",
    "\n",
    "# Categorical columns you want to preserve\n",
    "categorical_columns = ['nameOrig', 'nameDest', 'type']  # Add other categorical columns if needed\n",
    "\n",
    "# Step 3: Apply the synthetic generation for each transaction type without fraud\n",
    "synthetic_fraud_data = []\n",
    "\n",
    "for transaction_type in ['DEBIT', 'CASH_IN', 'PAYMENT']:\n",
    "    print(f\"Generating synthetic fraud data for {transaction_type}\")\n",
    "    \n",
    "    # Filter non-fraud cases for the specific transaction type\n",
    "    non_fraud_subset = non_fraud_data[non_fraud_data['type'] == transaction_type]\n",
    "    \n",
    "    if non_fraud_subset.empty:\n",
    "        print(f\"No non-fraud data for {transaction_type}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Generate synthetic fraud data for the type based on fraud cluster correlations\n",
    "    synthetic_data = generate_synthetic_data(non_fraud_subset, fraud_clusters, categorical_columns, n_samples=500)\n",
    "    \n",
    "    # Add the type column back (though it should be retained via categorical columns)\n",
    "    synthetic_data['type'] = transaction_type\n",
    "    \n",
    "    synthetic_fraud_data.append(synthetic_data)\n",
    "\n",
    "# Combine the synthetic fraud data\n",
    "if synthetic_fraud_data:\n",
    "    synthetic_fraud_data_combined = pd.concat(synthetic_fraud_data, ignore_index=True)\n",
    "\n",
    "    # Add synthetic fraud data to the original dataset\n",
    "    complete_dataset = pd.concat([non_fraud_data, fraud_data, synthetic_fraud_data_combined], ignore_index=True)\n",
    "    \n",
    "    print(\"Synthetic fraud data generation complete!\")\n",
    "else:\n",
    "    print(\"No synthetic fraud data was generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Method-2 SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:21:42.299604Z",
     "iopub.status.busy": "2024-10-08T20:21:42.299135Z",
     "iopub.status.idle": "2024-10-08T20:21:52.422170Z",
     "shell.execute_reply": "2024-10-08T20:21:52.420957Z",
     "shell.execute_reply.started": "2024-10-08T20:21:42.299548Z"
    }
   },
   "outputs": [],
   "source": [
    "#Identify the corelation among fields and types\n",
    "# One-Hot Encode the 'Type' column to include it in correlation\n",
    "data_encoded = pd.get_dummies(data, columns=['type'], drop_first=False)\n",
    " \n",
    "# Apply Label Encoding for 'nameOrig' and 'nameDest' columns\n",
    "label_encoder = LabelEncoder()\n",
    "data_encoded['nameOrig'] = label_encoder.fit_transform(data_encoded['nameOrig'])\n",
    "data_encoded['nameDest'] = label_encoder.fit_transform(data_encoded['nameDest'])\n",
    " \n",
    "# Select only numeric columns, which now include the one-hot encoded 'Type' columns and label-encoded 'nameOrig', 'nameDest'\n",
    "numeric_columns = data_encoded.select_dtypes(include=[float, int, bool])\n",
    " \n",
    "# Generate the correlation matrix\n",
    "correlation_matrix = numeric_columns.corr()\n",
    " \n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:21:52.423991Z",
     "iopub.status.busy": "2024-10-08T20:21:52.423588Z",
     "iopub.status.idle": "2024-10-08T20:21:54.048317Z",
     "shell.execute_reply": "2024-10-08T20:21:54.046937Z",
     "shell.execute_reply.started": "2024-10-08T20:21:52.423951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate mean and covariance\n",
    "def calculate_mean_cov(dataset):\n",
    "    numeric_data = dataset[['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFraud']]\n",
    "    mean = numeric_data.mean().values\n",
    "    cov = numeric_data.cov().values\n",
    "    return mean, cov\n",
    " \n",
    "# Function to ensure covariance matrix is positive semi-definite\n",
    "def nearest_positive_semidefinite(cov_matrix):\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "    eigvals = np.maximum(eigvals, 1e-10)\n",
    "    cov_matrix_psd = np.dot(eigvecs, np.dot(np.diag(eigvals), eigvecs.T))\n",
    "    return cov_matrix_psd\n",
    " \n",
    "# Function to generate synthetic data with SMOTE\n",
    "def generate_synthetic_data_with_smote(n_samples=10000, fraud_ratio=0.1, real_data=None):\n",
    "    mean, cov = calculate_mean_cov(real_data)\n",
    "    cov = nearest_positive_semidefinite(cov)  # Ensure cov is positive semi-definite\n",
    " \n",
    "    # Generate synthetic numeric data\n",
    "    synthetic_data = np.random.multivariate_normal(mean, cov, n_samples)\n",
    " \n",
    "    # Create DataFrame from synthetic data\n",
    "    df = pd.DataFrame(synthetic_data, columns=['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFraud'])\n",
    " \n",
    "    # Convert 'isFraud' to binary 0 or 1 based on fraud_ratio\n",
    "    df['isFraud'] = np.where(df['isFraud'] > np.percentile(df['isFraud'], 100 * (1 - fraud_ratio)), 1, 0)\n",
    " \n",
    "    # Add synthetic 'type' data\n",
    "    type_probabilities = [0.15, 0.3, 0.05, 0.35, 0.15]\n",
    "    df['type'] = np.random.choice(['CASH_IN', 'CASH_OUT', 'DEBIT', 'PAYMENT', 'TRANSFER'], size=n_samples, p=type_probabilities)\n",
    " \n",
    "    # One-hot encode the 'type' column\n",
    "    df = pd.get_dummies(df, columns=['type'], drop_first=False)\n",
    " \n",
    "    # Add original 'nameOrig' and 'nameDest' columns from the real dataset\n",
    "    df['nameOrig'] = real_data['nameOrig'].sample(n=n_samples, replace=True).values\n",
    "    df['nameDest'] = real_data['nameDest'].sample(n=n_samples, replace=True).values\n",
    " \n",
    "    # Exclude 'nameOrig' and 'nameDest' for scaling\n",
    "    X = df.drop(['isFraud', 'nameOrig', 'nameDest'], axis=1)\n",
    "    y = df['isFraud']\n",
    " \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    " \n",
    "    # Apply SMOTE to balance classes\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    " \n",
    "    # Rebuild the DataFrame after SMOTE\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled['isFraud'] = y_resampled\n",
    " \n",
    "    # Add 'nameOrig' and 'nameDest' back to the resampled DataFrame\n",
    "    df_resampled['nameOrig'] = df['nameOrig'].sample(n=len(df_resampled), replace=True).values\n",
    "    df_resampled['nameDest'] = df['nameDest'].sample(n=len(df_resampled), replace=True).values\n",
    " \n",
    "    return df_resampled\n",
    " \n",
    "# Generate synthetic data with SMOTE\n",
    "synthetic_data_smote = generate_synthetic_data_with_smote(n_samples=20000, fraud_ratio=0.2, real_data=data)\n",
    " \n",
    "# View the distribution\n",
    "print(synthetic_data_smote['isFraud'].value_counts())\n",
    " \n",
    "# Save the synthetic data\n",
    "synthetic_data_smote.to_csv('synthetic_fraud_detection_data_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:21:54.050462Z",
     "iopub.status.busy": "2024-10-08T20:21:54.049994Z",
     "iopub.status.idle": "2024-10-08T20:21:54.090243Z",
     "shell.execute_reply": "2024-10-08T20:21:54.089194Z",
     "shell.execute_reply.started": "2024-10-08T20:21:54.050407Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Calculate the counts of fraudulent transactions\n",
    "# Ensure the values in type columns are properly binary (0 or 1)\n",
    "# First, we'll inspect the unique values in these columns\n",
    "for col in ['type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']:\n",
    "    print(f\"Unique values in {col} before fixing:\", synthetic_data_smote[col].unique())\n",
    "\n",
    "# Clip the values in the type columns to ensure they are between 0 and 1\n",
    "synthetic_data_smote[['type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']] = \\\n",
    "    synthetic_data_smote[['type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']].apply(lambda x: x.clip(lower=0, upper=1))\n",
    "\n",
    "# Verify that the type columns now contain only 0 and 1\n",
    "for col in ['type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']:\n",
    "    print(f\"Unique values in {col} after fixing:\", synthetic_data_smote[col].unique())\n",
    "\n",
    "# Filter for fraudulent transactions\n",
    "fraudulent_transactions = synthetic_data_smote[synthetic_data_smote['isFraud'] == 1]\n",
    "\n",
    "# Calculate the counts of fraudulent transactions by summing the one-hot encoded type columns\n",
    "fraud_counts_by_type = pd.DataFrame({\n",
    "    'CASH_IN': fraudulent_transactions['type_CASH_IN'].sum(),\n",
    "    'CASH_OUT': fraudulent_transactions['type_CASH_OUT'].sum(),\n",
    "    'DEBIT': fraudulent_transactions['type_DEBIT'].sum(),\n",
    "    'PAYMENT': fraudulent_transactions['type_PAYMENT'].sum(),\n",
    "    'TRANSFER': fraudulent_transactions['type_TRANSFER'].sum()\n",
    "}, index=['Fraudulent Transactions']).T\n",
    "\n",
    "# Output the corrected summary table\n",
    "print(\"\\nFraudulent Transactions by Type:\")\n",
    "print(fraud_counts_by_type)\n",
    "\n",
    "# Total number of fraudulent transactions\n",
    "total_fraudulent_transactions = fraudulent_transactions['isFraud'].count()\n",
    "print(\"\\nTotal fraudulent transactions:\", total_fraudulent_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:21:54.092187Z",
     "iopub.status.busy": "2024-10-08T20:21:54.091717Z",
     "iopub.status.idle": "2024-10-08T20:21:55.830041Z",
     "shell.execute_reply": "2024-10-08T20:21:55.828740Z",
     "shell.execute_reply.started": "2024-10-08T20:21:54.092124Z"
    }
   },
   "outputs": [],
   "source": [
    "fraud_transactions = complete_dataset[complete_dataset['isFraud'] == 1]\n",
    "non_fraud_transactions = complete_dataset[complete_dataset['isFraud'] == 0]\n",
    "\n",
    "# Step 1: Prepare the dataset (include both fraud and non-fraud cases)\n",
    "fraud_data = complete_dataset[complete_dataset['isFraud'] == 1]\n",
    "non_fraud_data = complete_dataset[complete_dataset['isFraud'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.11 Changing categorical NameOrigin and NameDest to Int using Frequency Encoding as Label encoding for unique Ids is not a good approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:21:55.832204Z",
     "iopub.status.busy": "2024-10-08T20:21:55.831737Z",
     "iopub.status.idle": "2024-10-08T20:21:55.850173Z",
     "shell.execute_reply": "2024-10-08T20:21:55.848948Z",
     "shell.execute_reply.started": "2024-10-08T20:21:55.832162Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:21:55.851981Z",
     "iopub.status.busy": "2024-10-08T20:21:55.851584Z",
     "iopub.status.idle": "2024-10-08T20:22:20.656150Z",
     "shell.execute_reply": "2024-10-08T20:22:20.654948Z",
     "shell.execute_reply.started": "2024-10-08T20:21:55.851930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Frequency encoding for 'nameOrig' and 'nameDest'\n",
    "complete_dataset['nameOrig_freq'] = complete_dataset['nameOrig'].map(complete_dataset['nameOrig'].value_counts())\n",
    "complete_dataset['nameDest_freq'] = complete_dataset['nameDest'].map(complete_dataset['nameDest'].value_counts())\n",
    "\n",
    "# Drop the original 'nameOrig' and 'nameDest' columns\n",
    "complete_dataset.drop(['nameOrig', 'nameDest'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.12 Calculating Balance Change Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:22:20.663837Z",
     "iopub.status.busy": "2024-10-08T20:22:20.662753Z",
     "iopub.status.idle": "2024-10-08T20:22:22.259212Z",
     "shell.execute_reply": "2024-10-08T20:22:22.258037Z",
     "shell.execute_reply.started": "2024-10-08T20:22:20.663793Z"
    }
   },
   "outputs": [],
   "source": [
    "#You want to track the decrease in the balance after a transaction and its relation to fraud. You can create new features to capture these insights:\n",
    "#Balance decrease ratio: Measure how much the balance has decreased after the transaction.\n",
    "#balance_decrease_ratio=amountoldbalanceOrg−newbalanceOrig\n",
    "#This shows how much of the original balance was spent.\n",
    "#Zero balance flag: A feature that flags if the oldbalanceOrg is zero, which could indicate suspicious activity for certain transaction types.\n",
    "\n",
    "complete_dataset['balance_decrease_ratio'] = (complete_dataset['oldbalanceOrg'] - complete_dataset['newbalanceOrig']) / complete_dataset['amount']\n",
    "complete_dataset['is_old_balance_zero'] = complete_dataset['oldbalanceOrg'] == 0\n",
    "complete_dataset['balance_decrease_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "# Set NaN values in 'amount_to_oldbalance_ratio' to zero\n",
    "complete_dataset['balance_decrease_ratio'] = complete_dataset['balance_decrease_ratio'].fillna(0)\n",
    "\n",
    "#TRANSFER and CASH_OUT transactions might be more likely to involve fraud\n",
    "complete_dataset['is_type_transfer'] = complete_dataset['type'] == 'TRANSFER'\n",
    "complete_dataset['is_type_cash_out'] = complete_dataset['type'] == 'CASH_OUT'\n",
    "complete_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.13 Scaling/Normalization & One-Hot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:22:22.260817Z",
     "iopub.status.busy": "2024-10-08T20:22:22.260497Z",
     "iopub.status.idle": "2024-10-08T20:24:25.558356Z",
     "shell.execute_reply": "2024-10-08T20:24:25.557161Z",
     "shell.execute_reply.started": "2024-10-08T20:22:22.260782Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to scale/normalize\n",
    "columns_to_scale = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest','nameOrig_freq', 'nameDest_freq']\n",
    "\n",
    "if 'amount_to_oldbalance_ratio' in complete_dataset.columns:\n",
    "    complete_dataset.drop('amount_to_oldbalance_ratio', axis=1, inplace=True)\n",
    "    \n",
    "# Apply Standard scaling only to numerical columns\n",
    "scaler = StandardScaler()\n",
    "complete_dataset[columns_to_scale] = scaler.fit_transform(complete_dataset[columns_to_scale])\n",
    "\n",
    "# Ensure that after scaling, all columns (both scaled and categorical) are included in the dataset\n",
    "df_scaled = complete_dataset.copy()\n",
    "\n",
    "# Apply One-Hot Encoding on the 'type' column\n",
    "df_scaled = pd.get_dummies(df_scaled, columns=['type'], drop_first=True)\n",
    "\n",
    "# Retain labels ('isFraud', 'isFlaggedFraud') as they are (not scaled/normalized)\n",
    "X = df_scaled.drop(['isFraud', 'isFlaggedFraud'], axis=1)  # Features\n",
    "y = df_scaled['isFraud']  # Target variable\n",
    "\n",
    "# Optional: Check the dataset after scaling and encoding\n",
    "print(df_scaled.head())\n",
    "df_scaled.to_csv('/kaggle/working/Test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:24:25.560425Z",
     "iopub.status.busy": "2024-10-08T20:24:25.560033Z",
     "iopub.status.idle": "2024-10-08T20:27:53.620492Z",
     "shell.execute_reply": "2024-10-08T20:27:53.619068Z",
     "shell.execute_reply.started": "2024-10-08T20:24:25.560384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to DataFrames for saving\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "y_train_df = pd.DataFrame(y_train.values, columns=['isFraud'])\n",
    "y_test_df = pd.DataFrame(y_test.values, columns=['isFraud'])   \n",
    "\n",
    "os.makedirs('/kaggle/working/train_test_data', exist_ok=True)\n",
    "\n",
    "X_train_scaled_df.to_csv('/kaggle/working/train_test_data/X_train_scaled.csv', index=False)\n",
    "X_test_scaled_df.to_csv('/kaggle/working/train_test_data/X_test_scaled.csv', index=False)\n",
    "y_train_df.to_csv('/kaggle/working/train_test_data/y_train.csv', index=False)\n",
    "y_test_df.to_csv('/kaggle/working/train_test_data/y_test.csv', index=False)\n",
    "\n",
    "print(\"Training and testing data saved in /kaggle/working/train_test_data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.1 Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:27:53.622705Z",
     "iopub.status.busy": "2024-10-08T20:27:53.622250Z",
     "iopub.status.idle": "2024-10-08T20:32:25.794798Z",
     "shell.execute_reply": "2024-10-08T20:32:25.793697Z",
     "shell.execute_reply.started": "2024-10-08T20:27:53.622653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression with class balancing\n",
    "log_reg = LogisticRegression(class_weight='balanced',max_iter=500)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# AUC-ROC Score\n",
    "# Get the probabilities for the positive class (fraud)\n",
    "y_proba_log_reg = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_score = roc_auc_score(y_test, y_proba_log_reg)\n",
    "print(f\"AUC-ROC Score: {roc_score}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_log_reg)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_score:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"Cross-Validation AUC-ROC Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation AUC-ROC Score: {cv_scores.mean()}\")\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_filename = '/kaggle/working/Logistic_Regression_model.pkl'\n",
    "joblib.dump(log_reg, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.2 CatBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T20:32:25.797800Z",
     "iopub.status.busy": "2024-10-08T20:32:25.796712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize CatBoostClassifier with class balancing\n",
    "#catboost_model = CatBoostClassifier(class_weights=[1, 10], iterations=500, verbose=False)  \n",
    "\n",
    "catboost_params_optuna = {\n",
    "'depth':2,\n",
    " 'l2_leaf_reg': 0.0687356140243106,\n",
    " 'learning_rate':0.0888501063398756,\n",
    "'colsample_bylevel': 0.7017148360901887,\n",
    " 'subsample': 0.814719415526711,\n",
    "    'random_seed': 42,\n",
    "    'eval_metric': 'AUC',\n",
    "    'verbose': False,\n",
    "    'loss_function': 'Logloss',\n",
    "}\n",
    "# catboost model\n",
    "catboost_model = CatBoostClassifier(**catboost_params_optuna)\n",
    "# Train the CatBoost model\n",
    "catboost_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_catboost = catboost_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate CatBoost\n",
    "print(\"CatBoost Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_catboost))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_catboost))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_catboost))\n",
    "\n",
    "# AUC-ROC Score\n",
    "# Get the probabilities for the positive class (fraud)\n",
    "y_proba_catboost = catboost_model.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_score_catboost = roc_auc_score(y_test, y_proba_catboost)\n",
    "print(f\"AUC-ROC Score: {roc_score_catboost}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr_catboost, tpr_catboost, thresholds_catboost = roc_curve(y_test, y_proba_catboost)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_catboost, tpr_catboost, color='green', label=f\"ROC Curve (AUC = {roc_score_catboost:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve - CatBoost\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores_catboost = cross_val_score(catboost_model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"Cross-Validation AUC-ROC Scores: {cv_scores_catboost}\")\n",
    "print(f\"Mean Cross-Validation AUC-ROC Score: {cv_scores_catboost.mean()}\")\n",
    "\n",
    "# Save the trained CatBoost model to a file\n",
    "catboost_model_filename = '/kaggle/working/CatBoost_Model.pkl'\n",
    "joblib.dump(catboost_model, catboost_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.3 Decision Tree Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Decision Tree with class balancing\n",
    "decision_tree = DecisionTreeClassifier(class_weight='balanced', max_depth=10, random_state=42)\n",
    "decision_tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree = decision_tree.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "print(\"Decision Tree:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_tree))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tree))\n",
    "\n",
    "# AUC-ROC Score\n",
    "y_proba_tree = decision_tree.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_score_tree = roc_auc_score(y_test, y_proba_tree)\n",
    "print(f\"AUC-ROC Score: {roc_score_tree}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr_tree, tpr_tree, thresholds_tree = roc_curve(y_test, y_proba_tree)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_tree, tpr_tree, color='green', label=f\"ROC Curve (AUC = {roc_score_tree:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Decision Tree: Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores_tree = cross_val_score(decision_tree, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"Cross-Validation AUC-ROC Scores: {cv_scores_tree}\")\n",
    "print(f\"Mean Cross-Validation AUC-ROC Score: {cv_scores_tree.mean()}\")\n",
    "# Save the trained model to a file\n",
    "model_filename = '/kaggle/working/Ddecision_Tree_model.pkl'\n",
    "joblib.dump(decision_tree, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.4 Deep Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the model\n",
    "def build_fraud_detection_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First layer: Input layer\n",
    "    model.add(layers.Input(shape=input_shape))  # Define input shape using Input layer\n",
    "    \n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Number of random splits for validation\n",
    "n_splits = 5\n",
    "cv_scores = []\n",
    "\n",
    "# Train-Validate Split\n",
    "input_shape = (X_train_scaled.shape[1],)  # Input shape of the features\n",
    "\n",
    "for i in range(n_splits):\n",
    "    print(f\"\\nStarting fold {i+1}/{n_splits}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(\n",
    "        X_train_scaled, y_train, test_size=0.2, stratify=y_train, random_state=i)\n",
    "    \n",
    "    model = build_fraud_detection_model(input_shape)  # New model instance for each split\n",
    "    print(f\"Training on fold {i+1}...\")\n",
    "    \n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=1, batch_size=64, validation_data=(X_val_fold, y_val_fold), verbose=0)\n",
    "    \n",
    "    # Predict probabilities for the AUC-ROC\n",
    "    y_val_pred_proba = model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate ROC AUC score\n",
    "    auc_score = roc_auc_score(y_val_fold, y_val_pred_proba)\n",
    "    cv_scores.append(auc_score)\n",
    "    \n",
    "    # Print the results for this fold\n",
    "    print(f\"Fold {i+1} AUC-ROC Score: {auc_score:.4f}\")\n",
    "    print(f\"Fold {i+1} Training Loss: {history.history['loss'][-1]:.4f}, Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Final cross-validation AUC scores\n",
    "print(f\"\\nCross-Validation AUC Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation AUC Score: {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "print(\"\\nTraining the final model on the full dataset...\")\n",
    "model_final = build_fraud_detection_model(input_shape)\n",
    "history_final = model_final.fit(X_train_scaled, y_train, epochs=1, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "loss, accuracy = model_final.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict probabilities for the AUC-ROC\n",
    "y_pred_proba_final = model_final.predict(X_test_scaled)\n",
    "\n",
    "# ROC Curve and AUC Score\n",
    "roc_score = roc_auc_score(y_test, y_pred_proba_final)\n",
    "print(f\"AUC-ROC Score: {roc_score:.4f}\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_final)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_score:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_filename = '/kaggle/working/DNN_model.pkl'\n",
    "joblib.dump(model_final, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.5 Comparative Analysis of Pre-Trained Classification Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Iterate over models\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model (for demonstration; typically, you load a pre-trained model)\n",
    "    model.fit(X_test_scaled, y_test)  # Use pre-trained model loading here if applicable\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    print(\"AUC-ROC Score:\", auc_roc)\n",
    "\n",
    "    # Plotting False Positive Curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', label='ROC Curve')\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4) Autoencoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.1 This is Experiment to see how Autoencoder will work on this data**\n",
    "## Taking 20% of data as taking all data was giving Memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking 20% of whole data as on 63 lakh records it was not running\n",
    "sample_data = df_scaled.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Retain labels ('isFraud', 'isFlaggedFraud') as they are (not scaled/normalized)\n",
    "X = sample_data.drop(['isFraud', 'isFlaggedFraud'], axis=1)  # Features\n",
    "y = sample_data['isFraud']  # Target variable\n",
    "\n",
    "\n",
    "# Dividing in X train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.2 Build Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the AutoEncoder\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 16 # The size of the encoded representation\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Train the AutoEncoder\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=5, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "# Extract the encoder part of the AutoEncoder\n",
    "encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "# Encode the data (for downstream tasks)\n",
    "X_train_encoded = encoder_model.predict(X_train_scaled)\n",
    "X_test_encoded = encoder_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.2.1 Directly feed test data to pre train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the encoded data to build a Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_encoded, y_train)\n",
    "# Evaluate performance on the test set\n",
    "y_pred = rf_model.predict(X_test_encoded)\n",
    "print(\"Sub-Experiment 1 - Direct Use of Pretrained Model\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.2.2 Finetune the model (Freeze early layers and fine tune upper layers) and record test performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and convert data types to ensure they are numeric (float32)\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "# Ensure target labels are 1D (if necessary)\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "# Build a new model based on the pretrained encoder\n",
    "for layer in encoder_model.layers:\n",
    "   layer.trainable = False  # Freeze the pretrained encoder\n",
    "# Add new layers for classification on top of the frozen encoder\n",
    "encoded_input = encoder_model.output\n",
    "x = Dense(32, activation='relu')(encoded_input)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "fine_tune_model = Model(inputs=encoder_model.input, outputs=output)\n",
    "# Compile and fine-tune the model\n",
    "fine_tune_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fit the model on the training data\n",
    "fine_tune_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "# Predict on the test data and evaluate the model\n",
    "y_pred = (fine_tune_model.predict(X_test) > 0.5).astype(int)\n",
    "# Print the evaluation results\n",
    "print(\"Sub-Experiment 2 - Fine-Tuned Model\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.2.3 Fully train single model and see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sub-Experiment 3: Fully Train a Model (No Pretraining)\n",
    "#Here, we train a model from scratch using the entire dataset (fully connected network).\n",
    "# Build a fully connected model from scratch\n",
    "input_layer = Input(shape=(X_train_scaled.shape[1],))\n",
    "x = Dense(64, activation='relu')(input_layer)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "full_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# Compile and train the model\n",
    "full_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "full_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "# Evaluate the model\n",
    "y_pred = (full_model.predict(X_test) > 0.5).astype(int)\n",
    "print(\"Sub-Experiment 3 - Fully Trained Model from Scratch\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **END**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Group-1 Members**\n",
    "- **Khushboo Gupta**\n",
    "- **Tanya Bansal**\n",
    "- **Gaurav Balyan**\n",
    "- **Subhadeep Roy**\n",
    "- **Rakkiappan P**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5655040,
     "sourceId": 9332838,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
